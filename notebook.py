# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r8eyg6wnxof4WcseAVKXR73Us0nw27ER

# ✅ Checkpoint 01 — Data Science & ML
**Datasets:**
- Individual Household Electric Power Consumption (UCI 00235)
- Appliances Energy Prediction (UCI 00374)

# PARTE 1 — Individual Household Electric Power Consumption (UCI 00235)
**Fonte (para uso no código):**
- Link dataset: `https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip`

**Observações importantes:**
- O dataset utiliza **;** como separador.
- Valores ausentes aparecem como **?**.
- As colunas principais incluem: `Global_active_power`, `Global_reactive_power`, `Voltage`, `Global_intensity`, `Sub_metering_1..3`.

## 1) Carregamento do dataset e exibição das 10 primeiras linhas
"""

import io, zipfile, urllib.request
import pandas as pd
import numpy as np


url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip"

with urllib.request.urlopen(url) as resp:
    zf = zipfile.ZipFile(io.BytesIO(resp.read()))
    inner_name = [n for n in zf.namelist() if n.endswith('.txt')][0]
    with zf.open(inner_name) as f:
        df = pd.read_csv(f, sep=';', na_values=['?'], low_memory=False)

df.head(10)

"""## 2) Diferença entre `Global_active_power` e `Global_reactive_power`

Global_active_power (kW): potência **ativa** consumida, ou seja, a parte da energia realmente convertida em trabalho útil (calor, luz, movimento).
    
Global_reactive_power (kVAR): potência **reativa**, associada a campos elétricos e magnéticos em cargas indutivas/capacitivas; não realiza trabalho útil diretamente, mas circula entre fonte e carga.

## 3) Verificar valores ausentes e quantificá-los
"""

missing_counts = df.isna().sum().sort_values(ascending=False)
display(missing_counts.to_frame("missing"))
print("\nTotal de ausentes:", int(missing_counts.sum()))

"""## 4) Converter `Date` para datetime e criar coluna `Weekday`"""

# O formato do dataset é 'dd/mm/yyyy'
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')

# Nova coluna: dia da semana
df['Weekday'] = df['Date'].dt.day_name()

display(df[['Date', 'Time', 'Weekday']].head(10))

"""## 5) Filtrar registros de 2007 e calcular média de consumo **diário** de `Global_active_power`"""

# Criar uma coluna datetime combinando Date+Time para permitir reamostragem
dt = pd.to_datetime(df['Date'].dt.strftime('%Y-%m-%d') + ' ' + df['Time'], errors='coerce')
df['DateTime'] = dt

df_2007 = df[(df['DateTime'].dt.year == 2007)].copy()

daily_mean_gap_2007 = (
    df_2007.set_index('DateTime')['Global_active_power']
          .astype(float)
          .resample('D')
          .mean()
)

print("Média diária de Global_active_power (2007) — primeiras linhas:")
display(daily_mean_gap_2007.head())
print("\nMédia geral (2007):", daily_mean_gap_2007.mean())

"""## 6) Gráfico de linha — variação de `Global_active_power` em um único dia (ex.: 2007-01-15)"""

import matplotlib.pyplot as plt


dt = pd.to_datetime(df['Date'].dt.strftime('%Y-%m-%d') + ' ' + df['Time'], errors='coerce')
df['DateTime'] = dt

one_day = '2007-01-15'
mask_day = (df['DateTime'].dt.date == pd.to_datetime(one_day).date())
series_day = df.loc[mask_day, ['DateTime', 'Global_active_power']].dropna()
series_day = series_day.set_index('DateTime')['Global_active_power'].astype(float)

plt.figure()
series_day.plot()
plt.title(f"Global_active_power ao longo do dia {one_day}")
plt.xlabel("Hora")
plt.ylabel("kW")
plt.show()

"""## 7) Histograma de `Voltage` e observações sobre a distribuição"""

voltage = pd.to_numeric(df['Voltage'], errors='coerce').dropna()

plt.figure()
plt.hist(voltage, bins=50)
plt.title("Distribuição de Voltage")
plt.xlabel("Voltage (V)")
plt.ylabel("Frequência")
plt.show()

print("Observação: Em geral, a distribuição de 'Voltage' tende a ser unimodal, concentrada em torno da tensão nominal,")
print("com pequena dispersão e possíveis caudas finas devido a variações na rede elétrica.")

"""## 8) Consumo médio por **mês** em todo o período"""

monthly_mean_gap = (
    df.set_index('DateTime')['Global_active_power']
            .astype(float)
            .resample('M')
            .mean()
)
display(monthly_mean_gap.to_frame("mean_Global_active_power").head())

"""## 9) Dia com **maior** consumo de energia ativa global"""

daily_sum_gap = (
    df.set_index('DateTime')['Global_active_power']
            .astype(float)
            .resample('D')
            .sum(min_count=1)
)

max_day = daily_sum_gap.idxmax()
print("Dia com maior consumo (soma de GAP):", max_day.date(), "— valor:", daily_sum_gap.max())

"""## 10) Consumo médio em **dias de semana** vs **finais de semana**"""

df['is_weekend'] = df['DateTime'].dt.weekday >= 5

mean_weekday = (
    df[~df['is_weekend']].set_index('DateTime')['Global_active_power'].astype(float).resample('D').mean().mean()
)
mean_weekend = (
    df[df['is_weekend']].set_index('DateTime')['Global_active_power'].astype(float).resample('D').mean().mean()
)

print("Média diária (dias de semana):", mean_weekday)
print("Média diária (finais de semana):", mean_weekend)

"""## 11) Correlação entre `Global_active_power`, `Global_reactive_power`, `Voltage`, `Global_intensity`"""

cols = ['Global_active_power','Global_reactive_power','Voltage','Global_intensity']
corr_df = df[cols].apply(pd.to_numeric, errors='coerce').corr()
display(corr_df)

"""## 12) Nova variável `Total_Sub_metering` = soma de `Sub_metering_1..3`"""

for c in ['Sub_metering_1','Sub_metering_2','Sub_metering_3']:
    df[c] = pd.to_numeric(df[c], errors='coerce')

df['Total_Sub_metering'] = df[['Sub_metering_1','Sub_metering_2','Sub_metering_3']].sum(axis=1)

display(df[['Sub_metering_1','Sub_metering_2','Sub_metering_3','Total_Sub_metering']].head())

"""## 13) Mês em que `Total_Sub_metering` ultrapassa a **média** de `Global_active_power`"""

gap_mean = pd.to_numeric(df['Global_active_power'], errors='coerce').mean()

monthly_total_sub = (
    df.set_index('DateTime')['Total_Sub_metering']
            .resample('ME')
            .mean()
)

months_exceed = monthly_total_sub[monthly_total_sub > gap_mean]
print("Meses em que a média mensal de Total_Sub_metering > média geral de Global_active_power:")
display(months_exceed)

"""## 14) Série temporal de `Voltage` para **2008**"""

voltage_2008 = df[df['DateTime'].dt.year == 2008].set_index('DateTime')['Voltage'].astype(float).resample('D').mean()

plt.figure()
voltage_2008.plot()
plt.title("Voltage — média diária em 2008")
plt.xlabel("Data")
plt.ylabel("Voltage (V)")
plt.show()

"""## 15) Comparar consumo entre meses de **verão** e **inverno** (hemisfério norte)"""

# Hemisfério norte: verão ~ Jun-Set; inverno ~ Dez-Mar (aproximação)
series = df.set_index('DateTime')['Global_active_power'].astype(float).resample('D').mean()

summer_months = [6,7,8,9]
winter_months = [12,1,2,3]

summer_mean = series[series.index.month.isin(summer_months)].mean()
winter_mean = series[series.index.month.isin(winter_months)].mean()

print("Média diária no verão (HN):", summer_mean)
print("Média diária no inverno (HN):", winter_mean)

"""## 16) Amostragem aleatória de **1%** e comparação de distribuição de `Global_active_power`"""

# Converter para numérico e remover NA
gap_full = pd.to_numeric(df['Global_active_power'], errors='coerce').dropna()

# Amostra 1% iniciais usando iloc
n = int(0.01 * len(gap_full))  # 1% do total
sample_1pct = gap_full.iloc[0:n]

# Estatísticas
print("Média:", sample_1pct.mean())
print("Mediana:", sample_1pct.median())
print("Desvio padrão:", sample_1pct.std())

# Gráfico de distribuição
import matplotlib.pyplot as plt

plt.figure()
plt.hist(gap_full, bins=60, alpha=0.5, label='Base completa', density=True)
plt.hist(sample_1pct, bins=60, alpha=0.5, label='Amostra 1%', density=True)
plt.title("Distribuição de Global_active_power — base completa vs 1%")
plt.xlabel("kW")
plt.ylabel("Densidade")
plt.legend()
plt.show()

"""## 17) Normalização (Min-Max Scaling) das variáveis numéricas principais"""

from sklearn.preprocessing import MinMaxScaler

features_main = df[['Global_active_power','Global_reactive_power','Voltage','Global_intensity']].apply(pd.to_numeric, errors='coerce').dropna()
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features_main)
print("Shape após scaling:", features_scaled.shape)

"""## 18) K-Means — segmentar **dias** em 3 grupos de consumo"""

from sklearn.cluster import KMeans

# Agregar por dia (médias diárias dessas variáveis)
daily_feats = (
    df.set_index('DateTime')[['Global_active_power','Global_reactive_power','Voltage','Global_intensity']]
            .apply(pd.to_numeric, errors='coerce')
            .resample('D')
            .mean()
            .dropna()
)

km = KMeans(n_clusters=3, n_init=10, random_state=42)
clusters = km.fit_predict(daily_feats)

daily_feats_clustered = daily_feats.copy()
daily_feats_clustered['cluster'] = clusters
display(daily_feats_clustered.head())

cluster_summary = daily_feats_clustered.groupby('cluster').mean()
print("Resumo por cluster (médias diárias):")
display(cluster_summary)

"""## 19) Decomposição de série temporal (6 meses) — `Global_active_power`"""

from statsmodels.tsa.seasonal import seasonal_decompose

# Escolher uma janela de 6 meses (ex.: 2007-01 a 2007-06), com frequência diária
gap_daily = (
    df.set_index('DateTime')['Global_active_power']
            .apply(pd.to_numeric, errors='coerce')
            .resample('D').mean()
            .dropna()
)

six_months = gap_daily[(gap_daily.index >= '2007-01-01') & (gap_daily.index <= '2007-06-30')]

decomp = seasonal_decompose(six_months, model='additive', period=7)  # sazonalidade semanal aproximada

fig = decomp.plot()
fig.set_size_inches(10, 8)

"""## 20) Regressão linear simples — prever `Global_active_power` a partir de `Global_intensity`"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

df_reg = df[['Global_active_power','Global_intensity']].apply(pd.to_numeric, errors='coerce').dropna()

X = df_reg[['Global_intensity']].values
y = df_reg['Global_active_power'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("Coeficiente:", lr.coef_[0], "Intercepto:", lr.intercept_)
print("RMSE:", rmse, "R²:", r2)

# Curva ajustada (amostra)
import matplotlib.pyplot as plt
plt.figure()
plt.scatter(X_test[:2000], y_test[:2000], s=3, alpha=0.5, label="Real")
plt.plot(np.sort(X_test[:2000], axis=0), lr.predict(np.sort(X_test[:2000], axis=0)), label="Linear fit")
plt.title("Regressão Linear: GAP ~ GI")
plt.xlabel("Global_intensity")
plt.ylabel("Global_active_power")
plt.legend()
plt.show()

"""# PARTE 2 — Exercícios adicionais (IHEPC)

## 21) Séries temporais por hora — índice datetime, reamostragem 1H e horários de maior consumo
"""

# Definir índice datetime
ihepc_dt = df.set_index('DateTime').sort_index()
gap_hourly = ihepc_dt['Global_active_power'].apply(pd.to_numeric, errors='coerce').resample('H').mean()

# Consumo médio por hora do dia (0..23) no período completo
by_hour = gap_hourly.groupby(gap_hourly.index.hour).mean()
display(by_hour.to_frame("mean_GAP"))

import matplotlib.pyplot as plt
plt.figure()
by_hour.plot()
plt.title("Consumo médio por hora do dia (GAP)")
plt.xlabel("Hora do dia")
plt.ylabel("kW (médio)")
plt.show()

peak_hours = by_hour.sort_values(ascending=False).head(5)
print("Top 5 horários de maior consumo médio (hora -> kW):")
display(peak_hours)

"""## 22) Autocorrelação do consumo — lags de 1h, 24h e 48h"""

def autocorr_at_lag(series, lag):
    return series.autocorr(lag=lag)

print("Autocorrelação 1h:", autocorr_at_lag(gap_hourly, 1))
print("Autocorrelação 24h:", autocorr_at_lag(gap_hourly, 24))
print("Autocorrelação 48h:", autocorr_at_lag(gap_hourly, 48))

print("\nSim, existem padrões diários repetidos, embora não sejam perfeitos. Não são iguais sempre, mas existe um padrão diário")

"""## 23) PCA (2 componentes) em GAP, GRP, Voltage, GI"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

X_pca = ihepc_dt[['Global_active_power','Global_reactive_power','Voltage','Global_intensity']].apply(pd.to_numeric, errors='coerce').dropna()
scaler_pca = StandardScaler()
X_scaled = scaler_pca.fit_transform(X_pca)

pca = PCA(n_components=2, random_state=42)
X_pc = pca.fit_transform(X_scaled)

print("Variância explicada por componente:", pca.explained_variance_ratio_)
print("Variância total explicada:", pca.explained_variance_ratio_.sum())

"""## 24) Visualização de clusters (K-Means) no espaço PCA"""

# Reaproveitar K-Means com 3 clusters nos mesmos registros usados no PCA
km2 = KMeans(n_clusters=3, n_init=10, random_state=42).fit(X_scaled)
labels = km2.labels_

import matplotlib.pyplot as plt
plt.figure()
plt.scatter(X_pc[:,0], X_pc[:,1], s=2, alpha=0.5, c=labels)
plt.title("PCA (2D) com clusters K-Means (3 grupos)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

print("Pergunta: os grupos se separam de forma clara? Visualmente, avalie a sobreposição entre clusters no plano PC1 x PC2.")

print("""Resposta: Observando o gráfico, os grupos não se separam de forma clara. Há uma sobreposição significativa, principalmente na área central,
 onde os três clusters - representados em amarelo, roxo e verde - estão densamente misturados.
O cluster verde à esquerda e o cluster amarelo à direita mostram uma separação mais distinta em suas extremidades, mas suas bordas internas se misturam consideravelmente com o cluster roxo central.
O cluster roxo parece atuar mais como uma zona de transição do que como um grupo claramente definido.
Visualmente, o alto grau de sobreposição sugere que o algoritmo K-Means não encontrou três clusters distintos e bem separados no plano PC1 x PC2.""")

"""## 25) Regressão linear simples vs polinomial (grau 2): GAP ~ Voltage"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

df_reg2 = ihepc_dt[['Global_active_power','Voltage']].apply(pd.to_numeric, errors='coerce').dropna()
X = df_reg2[['Voltage']].values
y = df_reg2['Global_active_power'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear
lr_lin = LinearRegression().fit(X_train, y_train)
y_pred_lin = lr_lin.predict(X_test)
rmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))

# Polinomial grau 2
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

lr_poly = LinearRegression().fit(X_train_poly, y_train)
y_pred_poly = lr_poly.predict(X_test_poly)
rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))

print("RMSE Linear:", rmse_lin)
print("RMSE Polinomial (grau 2):", rmse_poly)

# Curvas (amostra)
xs = np.linspace(X_test.min(), X_test.max(), 200).reshape(-1,1)
ys_lin = lr_lin.predict(xs)
ys_poly = lr_poly.predict(poly.transform(xs))

import matplotlib.pyplot as plt
plt.figure()
plt.scatter(X_test[:3000], y_test[:3000], s=3, alpha=0.5, label="Real")
plt.plot(xs, ys_lin, label="Linear")
plt.plot(xs, ys_poly, label="Polinomial (g2)")
plt.title("GAP ~ Voltage: Linear vs Polinomial")
plt.xlabel("Voltage")
plt.ylabel("Global_active_power")
plt.legend()
plt.show()

"""# PARTE 3 — Appliances Energy Prediction (UCI 00374)
**Arquivo direto:** `https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv`

## 26) Carregamento e inspeção inicial
"""

appliances_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv"
df_app = pd.read_csv(appliances_url)

print("Dimensão:", df_app.shape)
print("\n.info():")
display(df_app.info())
print("\n.describe():")
display(df_app.describe(include='all').transpose().head(20))

# Converter timestamp se existir a coluna 'date'
if 'date' in df_app.columns:
    df_app['date'] = pd.to_datetime(df_app['date'], errors='coerce')

"""## 27) Distribuição do consumo (Appliances) — histogramas e série temporal"""

import matplotlib.pyplot as plt

if 'Appliances' in df_app.columns:
    plt.figure()
    plt.hist(df_app['Appliances'].dropna(), bins=50)
    plt.title("Distribuição de Appliances")
    plt.xlabel("Consumo (Wh)")
    plt.ylabel("Frequência")
    plt.show()

    if 'date' in df_app.columns:
        ts_app = df_app.set_index('date')['Appliances'].resample('H').mean()
        plt.figure()
        ts_app.plot()
        plt.title("Série temporal (média por hora) de Appliances")
        plt.xlabel("Data/Hora")
        plt.ylabel("Consumo (Wh)")
        plt.show()

    print("Pergunta: o consumo tende a se concentrar em valores mais baixos; picos indicam eventos específicos de uso.")

"""## 28) Correlações entre `Appliances` e variáveis ambientais"""

# Variáveis ambientais candidatas (comuns no dataset): T1..T9, RH1..RH9, T_out, RH_out, Press_mm_hg, Windspeed, Visibility, Tdewpoint
num_cols = df_app.select_dtypes(include=[float, int]).columns.tolist()
if 'Appliances' in num_cols:
    corr_env = df_app[num_cols].corr()['Appliances'].sort_values(ascending=False)
    display(corr_env.to_frame("corr_with_Appliances").head(15))
    display(corr_env.to_frame("corr_with_Appliances").tail(15))
    print("Pergunta: fatores com maior |correlação| (positiva/negativa) tendem a ter mais relação com o consumo.")
else:
    print("Coluna 'Appliances' não é numérica ou está ausente.")

"""## 29) Normalização (Min-Max Scaling) nas variáveis numéricas"""

from sklearn.preprocessing import MinMaxScaler

num_cols = df_app.select_dtypes(include=[float, int]).columns.tolist()
scaler2 = MinMaxScaler()
df_app_scaled = df_app.copy()
df_app_scaled[num_cols] = scaler2.fit_transform(df_app[num_cols])
print("Normalização concluída para", len(num_cols), "variáveis numéricas.")

"""## 30) PCA (2 componentes) — plot em 2D"""

from sklearn.decomposition import PCA

# Selecionar subconjunto de features comuns (evitar data/target)
features_app = [c for c in num_cols if c != 'Appliances']
X2 = df_app_scaled[features_app].dropna().values

pca2 = PCA(n_components=2, random_state=42)
X2_pc = pca2.fit_transform(X2)

print("Variância explicada:", pca2.explained_variance_ratio_, "— total:", pca2.explained_variance_ratio_.sum())

import matplotlib.pyplot as plt
plt.figure()
plt.scatter(X2_pc[:,0], X2_pc[:,1], s=2, alpha=0.4)
plt.title("PCA (2D) — Appliances dataset")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

print("Pergunta: não é possível identificar padrões ou agrupamentos naturais distintos.")

"""## 31) Regressão Linear Múltipla — prever `Appliances` pelas variáveis ambientais"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

if 'Appliances' in df_app.columns:
    # Usar variáveis numéricas exceto o alvo
    features = [c for c in num_cols if c != 'Appliances']
    X = df_app[features].fillna(df_app[features].median())
    y = df_app['Appliances']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    lr3 = LinearRegression().fit(X_train, y_train)
    y_pred = lr3.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f"Regressão Linear Múltipla:")
    print(f"- Erro médio da previsão (RMSE): {rmse:.2f}")
    print(f"- Qualidade do ajuste (R²): {r2:.2f} — quanto mais próximo de 1, melhor")
else:
    print("Coluna 'Appliances' ausente.")

"""## 32) Random Forest Regressor — prever `Appliances`"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

if 'Appliances' in df_app.columns:
    X = df_app[features].fillna(df_app[features].median())
    y = df_app['Appliances']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)

    rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
    r2_rf = r2_score(y_test, y_pred_rf)

    print("Random Forest Regressor:")
    print(f"- Erro médio da previsão (RMSE): {rmse_rf:.2f}")
    print(f"- Qualidade do ajuste (R²): {r2_rf:.2f} — quanto mais próximo de 1, melhor")

else:
    print("Coluna 'Appliances' ausente.")

"""## 33) K-Means clustering (k=3..5) — perfis de consumo"""

from sklearn.cluster import KMeans
import numpy as np

if 'Appliances' in df_app_scaled.columns:
    results = {}
    for k in [3,4,5]:
        km = KMeans(n_clusters=k, n_init=10, random_state=42)
        labels = km.fit_predict(df_app_scaled[features])
        results[k] = labels
        print(f"Modelo KMeans com k={k}")
        print(f"- Inércia (quanto menor, melhor): {km.inertia_:.2f}\n")

    # Analisar perfis médios por cluster (k=3 como exemplo)
    labels3 = results[3]
    df_k3 = df_app.copy()
    df_k3['cluster'] = labels3
    profiles = df_k3.groupby('cluster')['Appliances'].agg(['mean','median','count']).sort_index()
    print("=== Perfis de consumo por cluster (k=3) ===")
    display(profiles)
else:
    print("Base normalizada ausente.")

"""## 34) Classificação binária — alto vs baixo consumo (mediana)"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Import missing metrics


if 'Appliances' in df_app.columns:
    median_val = df_app['Appliances'].median()
    y_bin = (df_app['Appliances'] > median_val).astype(int)  # 1=alto, 0=baixo

    X = df_app[features].fillna(df_app[features].median())

    X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2, random_state=42)

    logreg = LogisticRegression(max_iter=1000, n_jobs=-1)
    logreg.fit(X_train, y_train)
    y_pred_lr = logreg.predict(X_test)

    rf_clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)
    rf_clf.fit(X_train, y_train)
    y_pred_rf = rf_clf.predict(X_test)

    # Logistic Regression
    print("=== Logistic Regression ===")
    print(f"Acurácia: {accuracy_score(y_test, y_pred_lr):.2f}")
    print("\nRelatório de Classificação:")
    print(classification_report(y_test, y_pred_lr))
    print("Matriz de Confusão:")
    print(confusion_matrix(y_test, y_pred_lr), "\n")

    # Random Forest
    print("=== Random Forest Classifier ===")
    print(f"Acurácia: {accuracy_score(y_test, y_pred_rf):.2f}")
    print("\nRelatório de Classificação:")
    print(classification_report(y_test, y_pred_rf))
    print("Matriz de Confusão:")
    print(confusion_matrix(y_test, y_pred_rf))
else:
    print("Coluna 'Appliances' ausente.")

"""## 35) Avaliação de classificação — matriz de confusão e métricas"""

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

def eval_cls(y_true, y_pred, label):
    print(f"\n=== {label} ===")
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred, zero_division=0))
    print("Recall:", recall_score(y_true, y_pred, zero_division=0))
    print("F1:", f1_score(y_true, y_pred, zero_division=0))
    print("Matriz de confusão:\n", confusion_matrix(y_true, y_pred))

eval_cls(y_test, y_pred_lr, "Logistic Regression")
eval_cls(y_test, y_pred_rf, "Random Forest Classifier")

print("\nPergunta: Tanto a Regrassão Logistica quanto o Random Forest cometem mais erros ao prever a classe 'alto consumo' (Appliences > mediana).")